## 第二章 神经网络的数学基础
### 2.0 概览 
> 张量，张量运算，微分，梯度下降，反向传播等
>
> 注：文中所写的代码只是笔者个人觉得要注意。并不全，需要完整代码的请参考书籍。
>
> 要显示数学公式请给浏览器装插件MathJax Plugin for Github（需要fq）

### 2.1 初识神经网络
#### 1.数据集
mnist数据集：将手写数字的灰度图像（28\*28像素）划分到10个类别里。  
测试集包含60000训练图像和10000张测试图像    

```python
from keras.datasets import mnist
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```

```py
>>> train_images.shape
(60000, 28, 28)
>>> len(train_labels)
60000
>>> train_labels
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
```



**类和标签的说明**
在分类问题中某个类别叫做类（class)，数据点叫做样本（sample）,某个样本对应的类叫做标签（label）

> 此数据集加载数据时进行分割(train_images,train_labels),(test_images,test_labels)=mnist.load_data()

#### 2.网络架构

**核心是构造层和编译**   

**层(layer)**,是一种数据处理模块，可以将他看成一个数据过滤器，进去一些数据，出来的数据更加有用。而神经网络就是有很多这样的层结构组成的数据处理的“筛子”   

本例中使用两个Dense层，也叫全连接层，最后一层为softmax层，是输出层，作用是将原本的输出值转换为概率值（总和为1），每一个概率表示分属于每一个类别的概率。本例有十个类别。 

```python
from keras import models
from keras import layers
network = models.Sequential()
network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))
network.add(layers.Dense(10, activation='softmax'))
```

  

**编译的三个重要参数**

- 损失函数（loss function):网络衡量训练数据性能的依据。即这次训练的结果好不好，有多好。  
- 优化器（optimizer）：根据训练数据和损失函数来更新网络的机制。(更新网络即调整参数)
- 训练、测试需要监控的指标（metric）：本例只指定了精度（accuracy）  

```python
network.compile(optimizer='rmsprop',
loss='categorical_crossentropy',
metrics=['accuracy'])
```

#### 3.数据预处理

神经网络每一层的数据要求必须是二维的，即（samples,features），即需要把多维的特征占平成一维。至于是否需要标准化或者其他处理，看实际情况。  最后需要对标签进行分类编码。

> 本例将[0,255]的取值范围收缩在[0,1]范围之内。

```pytho
# 准备图像数据
train_images = train_images.reshape((60000, 28 * 28))
train_images = train_images.astype('float32') / 255
test_images = test_images.reshape((10000, 28 * 28))
test_images = test_images.astype('float32') / 255
# 标签分类编码
from keras.utils import to_categorical
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```

#### 4.开始训练

训练使用神经网络的fit方法。是一种拟合模型。需要注意几个参数。  

- epochs:只训练周期数，即一共要训练多少次，每一次都需要输入全部样本，但全部样本是分批次输入的，通过batch_size参数来指定，默认为32，一般是不需要变动的。  

```py
network.fit(train_images, train_labels, epochs=5, batch_size=128)
```

在测试集上评估性能，使用神经网络的evaluate方法。给出测试特征值和标签即可。  

```py
>>> test_loss, test_acc = network.evaluate(test_images, test_labels)
>>> print('test_acc:', test_acc)
test_acc: 0.9785
```

> 介绍两个概念：
>
> 1. 过拟合（overfit）：在测试集上的精度低于训练集。这通常是因为训练集上过分的拟合了某些特殊点，就好像如果在一堆有趋势的散点图里画趋势线时，过分去接触每个点，就会使得最后的趋势线弯弯曲曲，反而不能反映真是情况。本例子就属于这种
> 2. 欠拟合（underfit）：在训练集和测试集上表现都很差。因为训练的不够。

### 2.2 神经网络的数据表示

前面我们使用存储数据的numpy多维数据就是一种**张量**，这是一种**数据容器**，矩阵就是二维张量。将其推广至任意维度，就是各个维度的张量，比如三维就是一个体，需要注意，张量的维度称之为轴（axis）。

numpy使用ndim就可以查看轴的个数，轴的个数也叫阶。    

#### 2.2.1 标量（0D张量，D-demension即维度）

标量即一个数。

#### 2.2.2 向量（1D张量）

即一维数组。需要需注意的是，如果一个向量有5个元素，我们可以叫他为5D向量（不是5D张量），这里的维度就表示沿着某一个轴的元素个数，而非轴的个数。

#### 2.2.3 矩阵（2D张量）

矩阵的行(row)和列(column)分别对应轴0和轴1。

#### 2.2.4 3D及更高维度的张量

将多个矩阵组合成一个新的数组，可以得到一个3D 张量，你可以将其直观地理解为数字
组成的立方体。一般所用的都是0-5D张量。

#### 2.2.5 关键属性

- 轴的个数（阶）
- 形状（shape）：一个整数数组，记录了张量沿着每一个轴的维度大小（元素个数），标量的为空。
- 数据类型（dtype）：张量中所包含的数据类型。

```py
#给出张量train_images 的轴的个数，即ndim 属性。
>>> print(train_images.ndim)
3
#下面是它的形状。
>>> print(train_images.shape)
(60000, 28, 28)
#下面是它的数据类型，即dtype 属性。
>>> print(train_images.dtype)
uint8
```

这样解释比较好：（60000,28,28），有60000个矩阵，每一个矩阵包括28*28的数据，每一个矩阵都是灰度图像，这样60000个矩阵就构成了一个3D张量。

#### 2.2.6 在numpy中操作张量（非常重要）

对前面的例子来说，使用train_images[i]来选择沿着轴0，选择了特定的元素，这种操作叫对张量进行切片。

```pytho
>>> my_slice = train_images[10:100]
>>> print(my_slice.shape)
(90, 28, 28)
```

但是我觉得书上对于翻译的不是很准确，书上将上面的操作称之选择第10到100个数字。实际上是选择了第10到第100个矩阵，也就是本例样本。而非翻译所说的数字。下面的两种等价写法，更加证实了这一点。

```python
>>> my_slice = train_images[10:100, :, :]
>>> my_slice.shape
(90, 28, 28)
>>> my_slice = train_images[10:100, 0:28, 0:28]
>>> my_slice.shape
(90, 28, 28)
```

可以看到，实际上就是对于每一个轴分别进行切片，这样就可以得到任意我们想要的数据形式。

比如选出每张灰度图像右下角14*14的区域

```py
my_slice = train_images[:, 14:, 14:]
```

又比如选出图像中心的14*14的区域。使用负数索引：表示相对于末尾的相对位置，-1表示最后一个元素。-7表示倒数第七个元素。

```python
my_slice = train_images[:, 7:-7, 7:-7]
```

即：对于每一个矩阵，选择第8个到第22（28减去后面6个为索引22）个元素。**因为切片是左闭右开，所以切片的[a:b]就是选择的第a+1和第b个元素。**

#### 2.2.7 数据批量

前面我们说的batch_size就是指定批量。即每一次训练分多少批次输入样本。对于这种批量的轴0，称之为批量轴(batch axis)或者批量维度(batch dimension).而对于整个数据集的轴0，就有样本轴（samples axis)和样本维度(samples dimension)。

#### 2.2.8 现实张量

> 向量数据：2D 张量，形状为 (samples, features)。  
> 时间序列数据或序列数据：3D 张量，形状为 (samples, timesteps, features)。  
> 图像：4D张量，形状为(samples, height, width, channels)或(samples, channels,
> height, width)。  
> 视频：5D张量，形状为(samples, frames, height, width, channels)或(samples,
> frames, channels, height, width)。  

**向量张量**  

轴0表示样本轴，轴1表示特征轴。即向量张量就是一个二维的张量。（注意与向量表示1维张量区别）。

注意一个数据集用什么张量存储不是取决于某一个样本有几个特征，而是这些特征可不可以放在一个维度里。比如

> 人口统计数据集，其中包括每个人的年龄、邮编和收入。每个人可以表示为包含 3 个值
> 的向量，而整个数据集包含100 000 个人，因此可以存储在形状为(100000, 3) 的2D
> 张量中。  
> 文本文档数据集，我们将每个文档表示为每个单词在其中出现的次数（字典中包含
> 20 000 个常见单词）。每个文档可以被编码为包含20 000 个值的向量（每个值对应于
> 字典中每个单词的出现次数），整个数据集包含500 个文档，因此可以存储在形状为
> (500, 20000) 的张量中。

这两个例子就可以用2D张量存储。而上面的例子，每一个图像必须用矩阵来存储，那么他就是3D张量。

**时间序列数据或序列数据**

有些数据可能有时间需求或其他的类似需求。就可以将数据存储在3D张量里，一般序列都存在轴1上，比如

![alt 图片](.\images\time_data.jpg "图1")



- 股票价格数据集：每一分钟，记录三个特征（当前价格、前一分钟最高价格、前一分钟最低价格），这样每一分钟就是一个3D向量，一天就是个（390,3）的2D张量，一个交易日假设390分钟，那么250天的数据就可以存储为一个（250,390,3）的3D张量。
- 推文数据集：推特一条推文最大字长为280字符。那么对于一条推文而言，将其编码为280个字符的序列。（其实就是一个个的单词），每一个单词又是由128个字符组成的，这样一个单词采用（0，1）编码，即出现了某字符就填1，否则填0，这样一个单词就是一个128D向量，一条推文就是（280,128）的2D张量，而很多推文就是一个（n,280,128)的3D张量。

**图像数据**

图像一般这样存放。（样本，高度，宽度，颜色深度），即(samples,height,width,colir_depth),需要注意颜色深度的位置可以放最后（TensorFlow用的），也可以放在样本轴后（theano用）。  

灰度图像的颜色深度为1，可以省略，那么灰度图像的每一个样本就可以存放于一个2D张量中。  

再有，还会用通道来描述颜色深度，（channels)。

![alt 图片](.\images\4d-tensor-image.jpg "图2")

**视频数据**

视频数据是有一系列帧，每一帧都是一张彩色的图像。那么显然就是（samples，frames，height，width，color_depth).

### 2.3 神经网络的核心：张量运算

前面我们提到深度学习是由很多层构建起来的，每一层都相当在做变换，对数据做变换，数据是用张量存储的，那么我们所做的变换都是对张量所做的变换。

现在先探讨一些理论的知识，后面会给出张量运算的几何解释。

**理论的相关知识**

看例子的第一层处理  

```python
keras.layers.Dense(512,activation='relu')
```

这一层的代码解释为输入一个2D张量，返回另一个2D张量。具体的运算：  

> output=relu(dot(W,input)+b)  dot表示点积运算，relu(x)=max(x,0)

这里的input是一个2D张量，像这样,轴0表示samples，纵轴表示features
$$
\begin{bmatrix}
{a_{11}}&{a_{12}}&{a_{13}}&{\cdots}&{a_{1784}} \\  
{a_{21}}&{a_{22}}&{a_{23}}&{\cdots}&{a_{2784}} \\  
{\vdots}&{\vdots}&{\vdots}&{\ddots}&{\vdots} \\  
{a_{600001}}&{a_{600002}}&{a_{600003}}&{\cdots}&{a_{60000784}}
\end{bmatrix} \tag{1}
$$
而W也是2D张量，和b都是该层的属性，也是我们以后要学习的参数。

W是这样的一个东西，w的行数表示一个神经元有多少权重系数，这取决于输入数据即矩阵1的列数，而列数表示这一层要将这个数据转换为多少类别。比如这里的512。也就是神经元的个数。
$$
\begin{bmatrix}
{w_{11}}&{w_{12}}&{\cdots}&{w_{1512}} \\  
{w_{21}}&{w_{22}}&{\cdots}&{w_{2512}} \\  
{\vdots}&{\vdots}&{\ddots}&{\vdots} \\  
{w_{7841}}&{w_{7842}}&{\cdots}&{w_{784512}} 
\end{bmatrix} \tag{2}
$$
然后他们做点积：
$$
\begin{bmatrix}
{y_{11}}&{y_{12}}&{\cdots}&{y_{1512}} \\  
{y_{21}}&{y_{22}}&{\cdots}&{y_{2512}} \\  
{\vdots}&{\vdots}&{\ddots}&{\vdots} \\  
{y_{6w1}}&{y_{6w2}}&{\cdots}&{y_{6w512}} 
\end{bmatrix} \tag{3}
$$
再加上列向量b，同样b的行数等于矩阵1的列数。最后得到的矩阵再使用激活函数relu进行处理，就得到了这一层的输出2D张量（命名为矩阵4）。他将作为在下一层的张量运算的输入。

> 备注：后面几节阐述的张量点积类比矩阵点积就可以了。还需要注意的是广播和逐个元素处理。矩阵4和向量b是如何相加的呢？首先要做的就是广播。
>
> 1. 向较小的张量添加轴（叫作广播轴），使其ndim 与较大的张量相同。
> 2.  将较小的张量沿着新轴重复，使其形状与较大的张量相同。
>
> 比如（30,10）和（10，）相加会先将（10，）扩展为（1,10）*（想想为什么不是（10,1）*然后再将每一列复制为30次。就得到了（30，10）。这就叫做广播，他们就可以相加了。
>
> 另外一个就是逐个元素处理了，举个简单的例子，两个矩阵的逐个元素相乘，就是对应位置的元素乘起来，注意这不是矩阵做点乘。上面的relu就是逐元素处理。可以简单实现为  
>
> ```py
> def naive_relu(x):
> assert len(x.shape) == 2
> x = x.copy()
> for i in range(x.shape[0]):
> for j in range(x.shape[1]):
> x[i, j] = max(x[i, j], 0)
> return x
> ```
>
> 不过实际上numpy内部支持了这种批量操作。只需要简单一句就可以实现  
>
> ```py
> import numpy as np
> z = x + y
> z = np.maximum(z, 0.)
> ```
>
> 

#### 2.3.1 张量变形

之前使用的是展平层处理第一次输入的数据，因为神经网络要求输入数据必须是（samples，features）的。我们也可以使用numpy的reshape直接处理后在输入，即：  

```py
train_images = train_images.reshape((60000, 28 * 28))
```

#### 2.3.2 张量运算几何解释

张量中的某个元素可以看做某位几个空间的坐标，将其和原点连起来就是一个某维向量，这样对其运算就是在对向量做几何变换。

#### 2.3.3 深度学习的几何解释

神经网络有一系列的张量运算构成，那么神经网络就可以解释为高维空间里很复杂的几何变换，这种变换可以通过很多简单的步骤来实现。  

![alt 图片](.\images\神经网络的几何解释.jpg)

让纸球恢复平整就是机器学习的内容：为复杂的、高度折叠的数据流形找到简洁的表示。现在你应该能够很好地理解，为什么深度学习特别擅长这一点：它将复杂的几何变换逐步分解为一长串基本的几何变换，这与人类展开纸球所采取的策略大致相同。深度网络的每一层都通过变换使数据解开一点点——许多层堆叠在一起，可以实现非常复杂的解开过程。
